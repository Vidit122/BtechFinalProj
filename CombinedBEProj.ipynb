{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALLING LIBRARIES\n",
    "\n",
    "\n",
    "%pip install newspaper3k\n",
    "%pip install lxml_html_clean\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "# !pip install transformers torch\n",
    "\n",
    "%pip install rake_nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "%pip install sumy\n",
    "%pip install google-api-python-client\n",
    "\n",
    "%pip install selenium\n",
    "%pip install webdriver-manager\n",
    "%pip install bs4\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install wordcloud\n",
    "%pip install BeautifulSoup\n",
    "\n",
    "%pip install fastapi uvicorn pandas spacy pdfplumber python-docx\n",
    "%pip install python-multipart\n",
    "%pip install pymupdf\n",
    "%pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRAPPING ARTICLES/BLOGS USING NLP SCRAPPER\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from apiclient.discovery import build\n",
    "from newspaper import Article\n",
    "from datetime import datetime, timedelta\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Google API Key and Custom Search Engine ID (Replace these with your actual credentials)\n",
    "api_key = \"AIzaSyAdcQAuDk5NU36FrPnz7wMmAW6C7nvxOCQ\"  #AIzaSyDpsF0BdJ2OBEFnwL5nD_CTHewM5PpDfUA  #AIzaSyAdcQAuDk5NU36FrPnz7wMmAW6C7nvxOCQ\n",
    "cse_id = \"d79ec70642d91487e\"   # a7d06f59dee40471a\n",
    "\n",
    "# Load existing data\n",
    "try:\n",
    "    existing_df = pd.read_excel('ArticlesFinal.xlsx')\n",
    "except FileNotFoundError:\n",
    "    existing_df = pd.DataFrame(columns=['Date', 'Title', 'Author', 'Publication Date', 'Article Text', 'Link', 'Source URL', 'Keywords', 'Job Market Insights'])\n",
    "\n",
    "# Initialize a new DataFrame\n",
    "df = pd.DataFrame(columns=['Date', 'Title', 'Author', 'Publication Date', 'Article Text', 'Link', 'Source URL', 'Keywords', 'Job Market Insights'])\n",
    "\n",
    "# Google Custom Search setup\n",
    "query = 'future jobs market in computer science'     # emerging technologies in computing\n",
    "resource = build(\"customsearch\", 'v1', developerKey=api_key).cse()\n",
    "\n",
    "def extract_keywords(text):\n",
    "    \"\"\"Extract common keywords from article text.\"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    keywords = [word for word in tokens if word.isalpha()]\n",
    "    return FreqDist(keywords).most_common(10)\n",
    "\n",
    "def get_article_info(url, date):\n",
    "    \"\"\"Fetch and parse article metadata and content.\"\"\"\n",
    "    try:\n",
    "        article = Article(url, timeout=10)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "\n",
    "        title = article.title or \"Unknown Title\"\n",
    "        authors = \", \".join(article.authors) if article.authors else \"Unknown Authors\"\n",
    "        publish_date = article.publish_date.replace(tzinfo=None) if article.publish_date else None\n",
    "        text = article.text or None\n",
    "\n",
    "        # Extract keywords and identify trends\n",
    "        keywords = extract_keywords(text) if text else []\n",
    "        market_insights = \"Tech industry growth\" if \"tech\" in text.lower() else \"General job trends\"\n",
    "\n",
    "        # Add the data to the DataFrame\n",
    "        df.loc[len(df)] = [date, title, authors, publish_date, text, url, urlparse(url).netloc, keywords, market_insights]\n",
    "        print(url, \"data added\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL: {url}\")\n",
    "        print(e)\n",
    "\n",
    "def get_articles_by_date():\n",
    "    \"\"\"Fetch articles within a specific date range using Google Custom Search.\"\"\"\n",
    "    # Define date range (last 30 days)\n",
    "    today = datetime.today()\n",
    "    last_month = today - timedelta(days=6000)\n",
    "    sdate = last_month.strftime('%y%m%d')\n",
    "    edate = today.strftime('%y%m%d')\n",
    "    date_range = f\"{sdate}:{edate}\"\n",
    "    urls = []\n",
    "\n",
    "    # Fetch articles (Google Custom Search returns 10 results per page)\n",
    "    for i in range(1, 100, 10):  # Pagination: up to 100 results\n",
    "        result = resource.list(q=query, cx=cse_id, sort=f\"date:r:{date_range}\", start=i).execute()\n",
    "        urls += result.get('items', [])\n",
    "\n",
    "    print(f\"Total results fetched: {len(urls)}\")\n",
    "\n",
    "    # Process each URL\n",
    "    for item in tqdm(urls):\n",
    "        url = item.get('link')\n",
    "        # if url and url not in existing_df['Link'].values and url not in df['Link'].values:\n",
    "        get_article_info(url, today.strftime('%Y-%m-%d'))\n",
    "        # else:\n",
    "            # print(\"URL already processed:\", url)\n",
    "\n",
    "# Fetch articles\n",
    "get_articles_by_date()\n",
    "\n",
    "# Combine new and existing data\n",
    "final_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "\n",
    "# Convert publication date to datetime and sort\n",
    "final_df['Publication Date'] = pd.to_datetime(final_df['Publication Date'], errors='coerce')\n",
    "final_df = final_df.sort_values(by=['Publication Date'], ascending=True)\n",
    "\n",
    "# Save to Excel\n",
    "final_df.to_csv('ArticlesNew.csv', index=False)\n",
    "print(\"Data saved to ArticlesNew.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATING INSIGHTS ON THE BASIS OF SCRAPPED DATA\n",
    "\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from rake_nltk import Rake\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "rake1 = Rake()\n",
    "\n",
    "\n",
    "existing_df = pd.read_excel('ArticlesFinal.xlsx')\n",
    "\n",
    "# Define positive and negative keywords\n",
    "POSITIVE_WORDS = [\"growth\", \"increase\", \"rise\", \"demand\", \"surge\", \"expand\", \"boost\", \"upward\", \"accelerate\", \"improve\"]\n",
    "NEGATIVE_WORDS = [\"decline\", \"fall\", \"decrease\", \"drop\", \"slowdown\", \"recession\", \"cut\", \"downward\", \"stagnant\"]\n",
    "\n",
    "# Load spaCy's English model for NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def analyze_job_trend(article_text):\n",
    "    # Tokenize the article into sentences\n",
    "    sentences = sent_tokenize(article_text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    results = defaultdict(str)\n",
    "\n",
    "    # Apply named entity recognition (NER) using spaCy\n",
    "    doc = nlp(article_text)\n",
    "\n",
    "    # Extract job roles and technologies (assumed to be proper nouns or certain noun phrases)\n",
    "    job_roles = set([ent.text.lower() for ent in doc.ents if ent.label_ in [\"ORG\", \"PERSON\", \"GPE\", \"WORK_OF_ART\"]])\n",
    "    technologies = set([token.text.lower() for token in doc if token.pos_ == \"PROPN\" and token.text not in stop_words])\n",
    "\n",
    "    print(f\"Extracted Job Roles: {job_roles}\")\n",
    "    print(f\"Extracted Technologies: {technologies}\")\n",
    "\n",
    "\n",
    "    # Initialize a dictionary to store trend analysis\n",
    "    trend_analysis = defaultdict(lambda: {\"positive\": 0, \"negative\": 0, \"mentions\": 0})\n",
    "\n",
    "    # Analyze each sentence to count positive and negative mentions of job roles/technologies\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "\n",
    "        for word in filtered_words:\n",
    "            if word in POSITIVE_WORDS:\n",
    "                trend_analysis[word][\"positive\"] += 1\n",
    "            elif word in NEGATIVE_WORDS:\n",
    "                trend_analysis[word][\"negative\"] += 1\n",
    "            if word in job_roles or word in technologies:\n",
    "                trend_analysis[word][\"mentions\"] += 1\n",
    "\n",
    "    print(f\"Trend Analysis: {trend_analysis}\")  # Debugging line to show trend analysis\n",
    "\n",
    "    # Analyze trends for each extracted job role or technology\n",
    "    for entity, counts in trend_analysis.items():\n",
    "        positive_count = counts[\"positive\"]\n",
    "        negative_count = counts[\"negative\"]\n",
    "        mentions = counts[\"mentions\"]\n",
    "\n",
    "        # Apply weighted analysis (giving more weight to negative words)\n",
    "        weighted_positive = positive_count\n",
    "        weighted_negative = negative_count * 1.2  # Negative words have a 20% higher weight\n",
    "\n",
    "        if mentions > 0:\n",
    "            if weighted_positive > weighted_negative:\n",
    "                results[entity] = f\"Rise: Positive keywords ({positive_count}) outweigh negative keywords ({negative_count}) for '{entity}'.\"\n",
    "            elif weighted_negative > weighted_positive:\n",
    "                results[entity] = f\"Fall: Negative keywords ({negative_count}) outweigh positive keywords ({positive_count}) for '{entity}'.\"\n",
    "            else:\n",
    "                results[entity] = f\"Unclear: Positive and negative keywords are balanced for '{entity}'.\"\n",
    "\n",
    "        else:\n",
    "            results[entity] = f\"No Mention: The job role/technology '{entity}' was not mentioned in the article.\"\n",
    "\n",
    "    # Format the results to match the desired output\n",
    "    output = []\n",
    "    for entity, analysis in results.items():\n",
    "        if \"Rise\" in analysis:\n",
    "            output.append(f\"{entity}, rise\")\n",
    "        elif \"Fall\" in analysis:\n",
    "            output.append(f\"{entity}, fall\")\n",
    "\n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    article_text = (existing_df['Article Text'].iloc[56])\n",
    "    # article_text = \"From the invention of the wheel to advancements in metallurgy, history has been defined by transformative innovations that have reshaped how we live and work. Today, we stand at the threshold of another paradigm shift driven by emerging technologies such as Artificial Intelligence (AI), Blockchain, the Internet of Things (IoT), and Machine Learning (ML), all of which are powering the Web 3.0 revolution.\"\n",
    "\n",
    "\n",
    "    # Analyze trends dynamically\n",
    "    result = analyze_job_trend(article_text)\n",
    "    print(result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# new code\n",
    "\n",
    "\n",
    "def extract_context_with_trends(text, words, trend_words):\n",
    "\n",
    "  results = []\n",
    "  for word in words:\n",
    "    for i, w in enumerate(text.split()):\n",
    "      if w == word:\n",
    "        start_index = max(0, i - 15)\n",
    "        end_index = min(len(text.split()), i + 16)\n",
    "        context_before = \" \".join(text.split()[start_index:i])\n",
    "        context_after = \" \".join(text.split()[i+1:end_index])\n",
    "\n",
    "        trend = None\n",
    "        for trend_word in trend_words:\n",
    "          if trend_word in context_before or trend_word in context_after:\n",
    "            trend = trend_word\n",
    "            break\n",
    "\n",
    "        results.append((word, context_before, context_after, trend))\n",
    "  return results\n",
    "\n",
    "# Example usage:\n",
    "text = (existing_df['Article Text'].iloc[56]).lower()\n",
    "final_text = \"\"\n",
    "keyword_text = \"\"\n",
    "sentiment_keyword = []\n",
    "\n",
    "\n",
    "# text = \"From the invention of the wheel to advancements in metallurgy, history has been defined by transformative innovations that have reshaped how we live and work. Today, we stand at the threshold of another paradigm shift driven by emerging technologies such as Artificial Intelligence (AI), Blockchain, the Internet of Things (IoT), and Machine Learning (ML), all of which are powering the Web 3.0 revolution.\"\n",
    "# text = text.lower()\n",
    "\n",
    "sentences = sent_tokenize(article_text.lower())\n",
    "stop_words = set(stopwords.words('english'))\n",
    "results = defaultdict(str)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "    # Apply named entity recognition (NER) using spaCy\n",
    "doc = nlp(article_text)\n",
    "\n",
    "    # Extract job roles and technologies (assumed to be proper nouns or certain noun phrases)\n",
    "job_roles = set([ent.text.lower() for ent in doc.ents if ent.label_ in [\"ORG\", \"PERSON\", \"GPE\", \"WORK_OF_ART\"]])\n",
    "technologies = set([token.text.lower() for token in doc if token.pos_ == \"PROPN\" and token.text not in stop_words])\n",
    "trend_words = [r'growth in', r'incline in', r'taking over', r'increase in demand for', r'advancements in', r'emerging', r'increase', r'rise', r'growth', r'demand', r'higher', r'more', r'expand', r'boost', r'opportunity', r'gain', r'decrease', r'fall', r'decline', r'drop', r'lower', r'less', r'reduce', r'cut', r'struggle', r'lack', r'uptick', r'surge', r'boom', r'prosper', r'flourish', r'thrive', r'soar', r'escalate', r'accelerate', r'expansion', r'diversification', r'innovation', r'disruption', r'downturn', r'recession', r'contraction', r'shrinkage', r'retrenchment', r'layoff', r'redundancy', r'stagnation', r'job market', r'labor force', r'unemployment rate', r'hiring spree', r'skill shortage', r'remote work', r'gig economy', r'artificial intelligence', r'automation']\n",
    "\n",
    "context_list = extract_context_with_trends(text, technologies, trend_words)\n",
    "\n",
    "for word, before, after, trend in context_list:\n",
    "\n",
    "  sentiment = TextBlob(before + \" \" + after).sentiment.polarity\n",
    "  sentiment_label = \"Positive\" if sentiment > 0 else \"Negative\" if sentiment < 0 else \"Neutral\"\n",
    "  print(f\"Word: {word}\")\n",
    "  rake1.extract_keywords_from_text(after)\n",
    "  keywords1 = rake1.get_ranked_phrases()\n",
    "  rake2 = Rake()\n",
    "  rake2.extract_keywords_from_text(before)\n",
    "  keywords2 = rake2.get_ranked_phrases()\n",
    "\n",
    "  print(f\"Match: {word}\")\n",
    "  # print(f\"Before: {before}\")\n",
    "  # print(\"Extracted Keywords (before) by RAKE:\")\n",
    "  # print('Before:- ')\n",
    "  for kw in keywords2:\n",
    "    # print(\"-\", kw)\n",
    "    match = re.search(r'(\\d+)', kw)\n",
    "    if match:\n",
    "        percentage = int(match.group(1))\n",
    "        print(\"-\", percentage)\n",
    "    else:\n",
    "        percentage = 0\n",
    "  # print(f\"After: {after}\")\n",
    "  # print(\"Extracted Keywords (after) by RAKE:\")\n",
    "  # print('After:- ')\n",
    "  for kw in keywords1:\n",
    "    # print(\"-\", kw)\n",
    "    match = re.search(r'(\\d+)', kw)\n",
    "    if match:\n",
    "        percentage = int(match.group(1))\n",
    "        print(\"-\", percentage)\n",
    "    else:\n",
    "        percentage = 0\n",
    "  print(f\"Trend: {trend}\")\n",
    "\n",
    "  def custom_sentiment_analysis(text):\n",
    "\n",
    "    positive_keywords = [\n",
    "      r'growth in', r'incline in', r'taking over', r'increase in demand for', r'advancements in',\n",
    "      r'emerging', r'increase', r'rise', r'growth', r'demand', r'higher', r'more', r'expand',\n",
    "      r'boost', r'opportunity', r'gain', r'uptick', r'surge', r'boom', r'prosper', r'flourish',\n",
    "      r'thrive', r'soar', r'escalate', r'accelerate', r'expansion', r'diversification', r'innovation'\n",
    "  ]\n",
    "    negative_keywords = [\n",
    "      r'decrease', r'fall', r'decline', r'drop', r'lower', r'less', r'reduce', r'cut', r'struggle',\n",
    "      r'lack', r'downturn', r'recession', r'contraction', r'shrinkage', r'retrenchment', r'layoff',\n",
    "      r'redundancy', r'stagnation', r'unemployment rate'\n",
    "  ]\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "\n",
    "    for word in words:\n",
    "      if word in positive_keywords:\n",
    "        positive_count += 1\n",
    "      elif word in negative_keywords:\n",
    "        negative_count += 1\n",
    "\n",
    "    if positive_count > negative_count:\n",
    "      return \"Positive\"\n",
    "    elif negative_count > positive_count:\n",
    "      return \"Negative\"\n",
    "    else:\n",
    "      return \"Neutral\"\n",
    "\n",
    "  text = before + \" \" + word + \" \" + after\n",
    "  keyword_text = keyword_text + \" \" + word\n",
    "  final_text = final_text + \" \" + text\n",
    "  sentiment = custom_sentiment_analysis(text)\n",
    "  print(sentiment)\n",
    "\n",
    "  sentiment_keyword.append((word, sentiment, percentage))\n",
    "\n",
    "  print(\"-\" * 20)\n",
    "\n",
    "print(final_text)\n",
    "print(keyword_text)\n",
    "\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "def summarize_text(text, num_sentences=5):\n",
    "  parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "  summarizer = LexRankSummarizer()\n",
    "  summary = summarizer(parser.document, num_sentences)\n",
    "  return [str(sentence) for sentence in summary]\n",
    "\n",
    "# Example usage:\n",
    "summary = summarize_text(final_text, 3)\n",
    "print(summary)\n",
    "\n",
    "\n",
    "def find_context(text, keywords):\n",
    "  words = text.split()\n",
    "  results = []\n",
    "\n",
    "  for keyword in keywords:\n",
    "    for i, word in enumerate(words):\n",
    "      if word == keyword:\n",
    "        start_index = max(0, i - 15)\n",
    "        end_index = min(len(words), i + 16)\n",
    "        context_before = \" \".join(words[start_index:i])\n",
    "        context_after = \" \".join(words[i+1:end_index])\n",
    "        results.append((keyword, context_before, context_after))\n",
    "\n",
    "  return results\n",
    "\n",
    "# Example usage:\n",
    "# text = \"This is a sample text to test the function. Harvard is a prestigious university. AI is changing the world. ChatGPT is a powerful language model. Harvard Business Review is a renowned publication.\"\n",
    "keywords = [r'growth in', r'incline in', r'taking over', r'dropped', r'increase in demand for', r'advancements in', r'emerging', r'increase', r'rise', r'growth', r'demand', r'higher', r'more', r'expand', r'boost', r'opportunity', r'gain', r'decrease', r'fall', r'decline', r'drop', r'lower', r'less', r'reduce', r'cut', r'struggle', r'lack', r'uptick', r'surge', r'boom', r'prosper', r'flourish', r'thrive', r'soar', r'escalate', r'accelerate', r'expansion', r'diversification', r'innovation', r'disruption', r'downturn', r'recession', r'contraction', r'shrinkage', r'retrenchment', r'layoff', r'redundancy', r'stagnation', r'job market', r'labor force', r'unemployment rate', r'hiring spree', r'skill shortage', r'remote work', r'gig economy', r'artificial intelligence', r'automation', r'expected', r'grow', r'projected', r'fall']\n",
    "\n",
    "context_list = find_context(str(summary), keywords)\n",
    "\n",
    "for word, before, after in context_list:\n",
    "  print(f\"Word: {word}\")\n",
    "  print(f\"Before: {before}\")\n",
    "  print(f\"After: {after}\")\n",
    "  print(\"-\" * 20)\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def summarize_sentiment_with_percentage(sentiment_keyword):\n",
    "    # Dictionary to store sentiment scores and percentages\n",
    "    sentiment_scores = defaultdict(lambda: {\"score\": 0, \"percentage\": 0})\n",
    "\n",
    "    # Update scores and percentages based on input data\n",
    "    for word, sentiment, percentage in sentiment_keyword:\n",
    "        if sentiment == \"Positive\":\n",
    "            sentiment_scores[word][\"score\"] += 1\n",
    "        elif sentiment == \"Negative\":\n",
    "            sentiment_scores[word][\"score\"] -= 1\n",
    "        elif sentiment == \"Neutral\":\n",
    "            sentiment_scores[word][\"score\"] += 0  # Neutral has no effect\n",
    "\n",
    "        sentiment_scores[word][\"percentage\"] += percentage\n",
    "\n",
    "    # Calculate final percentages and prepare the output\n",
    "    results = {}\n",
    "    for word, data in sentiment_scores.items():\n",
    "        total_percentage = data[\"percentage\"]\n",
    "        sentiment_score = data[\"score\"]\n",
    "\n",
    "        if sentiment_score == 0:\n",
    "            final_percentage = 0  # Neutral case\n",
    "        else:\n",
    "            final_percentage = (total_percentage * sentiment_score) / abs(sentiment_score)\n",
    "\n",
    "        # Check if percentage exceeds 100\n",
    "        if abs(final_percentage) > 100:\n",
    "            final_percentage = 0\n",
    "\n",
    "        results[word] = {\n",
    "            \"sentiment_score\": sentiment_score,\n",
    "            \"final_percentage\": final_percentage\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "# Calculate sentiment summary with percentages\n",
    "sentiment_summary = summarize_sentiment_with_percentage(sentiment_keyword)\n",
    "\n",
    "# Print results\n",
    "for word, data in sentiment_summary.items():\n",
    "    print(f\"Word: {word}, Sentiment Score: {data['sentiment_score']}, Final Percentage: {data['final_percentage']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATING ROADMAPS FOR A SELECTIVE ROLE \n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('job_role_and_skills_required.csv')\n",
    "\n",
    "def calculate_skill_frequencies(df):\n",
    "    all_skills = df['job_skills'].str.split(', ').explode()  # Flatten all skills\n",
    "    return Counter(all_skills)\n",
    "\n",
    "def plot_roadmap_for_job_title_sorted(job_title, df):\n",
    "    skill_frequencies = calculate_skill_frequencies(df)\n",
    "    row = df[df['job_title'] == job_title]\n",
    "    if row.empty:\n",
    "        print(f\"No data found for job title: {job_title}\")\n",
    "        return\n",
    "\n",
    "    skills = row.iloc[0]['job_skills'].split(', ')\n",
    "    skills_sorted = sorted(skills, key=lambda skill: skill_frequencies.get(skill, 0), reverse=True)\n",
    "\n",
    "    print(f\"Frequencies: {skill_frequencies}\")\n",
    "    print(f\"Skills sorted: {skills_sorted}\")\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(len(skills_sorted) - 1):\n",
    "        G.add_edge(skills_sorted[i], skills_sorted[i + 1])\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    pos = nx.spring_layout(G, seed=42)  # Position nodes\n",
    "    nx.draw(\n",
    "        G, pos, with_labels=True, node_color=\"skyblue\", edge_color=\"gray\",\n",
    "        node_size=3000, font_size=10, font_weight=\"bold\", arrowsize=20\n",
    "    )\n",
    "    start_node = skills_sorted[0]\n",
    "    end_node = skills_sorted[-1]\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=[start_node], node_color=\"green\", node_size=3500)\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=[end_node], node_color=\"red\", node_size=3500)\n",
    "\n",
    "    # Add labels for start and end points\n",
    "    plt.text(pos[start_node][0], pos[start_node][1] + 0.1, \"Start\", fontsize=12, color=\"green\", fontweight=\"bold\")\n",
    "    plt.text(pos[end_node][0], pos[end_node][1] - 0.1, \"End\", fontsize=12, color=\"red\", fontweight=\"bold\")\n",
    "\n",
    "    plt.title(f\"Roadmap for {job_title} (Skills Sorted by Frequency)\", fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "# Example: Plot roadmap for \"Senior Machine Learning Engineer\"\n",
    "plot_roadmap_for_job_title_sorted(\"Machine Learning Infrastructure Engineer\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUTUBE VIDEOS RECOMMENDATION FOR LEARNING ANY PARTICULAR SKILL FROM THE ROADMAP OF SKILLS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from apiclient.discovery import build #pip install google-api-python-client\n",
    "from apiclient.errors import HttpError #pip install google-api-python-client\n",
    "import pandas as pd #pip install pandas\n",
    "import oauth2client.tools as oauthtools\n",
    "from importlib import reload\n",
    "# Set DEVELOPER_KEY to the API key value from the APIs & auth > Registered apps\n",
    "# tab of\n",
    "# https://cloud.google.com/console\n",
    "# Please ensure that you have enabled the YouTube Data API for your project.\n",
    "\n",
    "DEVELOPER_KEY = \"AIzaSyBEGN6VevXeubmxYO5vJXDQSQIpEp1j-bc\"   # AIzaSyBEGN6VevXeubmxYO5vJXDQSQIpEp1j-bc\n",
    "YOUTUBE_API_SERVICE_NAME = \"youtube\"\n",
    "YOUTUBE_API_VERSION = \"v3\"\n",
    "\n",
    "\n",
    "# Define the YouTube search function\n",
    "def youtube_search(words):\n",
    "    q = words\n",
    "    max_results = 50\n",
    "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=DEVELOPER_KEY)\n",
    "\n",
    "    search_response = youtube.search().list(q=q, type=\"video\", part=\"id,snippet\", maxResults=max_results).execute()\n",
    "\n",
    "    videos = {}\n",
    "    for search_result in search_response.get(\"items\", []):\n",
    "        if search_result[\"id\"][\"kind\"] == \"youtube#video\":\n",
    "            video_id = search_result[\"id\"][\"videoId\"]\n",
    "            video_title = search_result[\"snippet\"][\"title\"]\n",
    "            # Add video ID and title to the dictionary\n",
    "            videos[video_id] = video_title\n",
    "\n",
    "    # Get video statistics for the collected videos\n",
    "    s = ','.join(videos.keys())\n",
    "    videos_list_response = youtube.videos().list(id=s, part='id,statistics').execute()\n",
    "\n",
    "    res = []\n",
    "    for i in videos_list_response['items']:\n",
    "        video_id = i['id']\n",
    "        video_title = videos[video_id]\n",
    "        video_link = f\"https://www.youtube.com/watch?v={video_id}\"  # Construct video link\n",
    "\n",
    "        # Create a dictionary for the video details\n",
    "        temp_res = {\n",
    "            'v_id': video_id,\n",
    "            'v_title': video_title,\n",
    "            'v_link': video_link\n",
    "        }\n",
    "        temp_res.update(i['statistics'])  # Add video statistics\n",
    "        res.append(temp_res)\n",
    "\n",
    "    # Convert results to a DataFrame and save to a CSV file\n",
    "    df = pd.DataFrame.from_dict(res)\n",
    "    df.to_csv('YTvideos.csv', mode='a', encoding='utf-8', index=False)\n",
    "\n",
    "# Define search terms\n",
    "terms = [\"python full course tutorial\"]\n",
    "for term in terms:\n",
    "    youtube_search(term)\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('YTvideos.csv', encoding='utf-8')\n",
    "\n",
    "df1 = df.drop_duplicates(['v_title'])\n",
    "\n",
    "# df1.head()\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "df = pd.read_csv('YTvideos.csv')\n",
    "\n",
    "df['viewCount'] = pd.to_numeric(df['viewCount'], errors='coerce')\n",
    "df['likeCount'] = pd.to_numeric(df['likeCount'], errors='coerce')\n",
    "df['commentCount'] = pd.to_numeric(df['commentCount'], errors='coerce')\n",
    "\n",
    "df.dropna(subset=['viewCount', 'likeCount', 'commentCount'], inplace=True)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[['viewCount_norm', 'likeCount_norm', 'commentCount_norm']] = scaler.fit_transform(\n",
    "    df[['viewCount', 'likeCount', 'commentCount']]\n",
    ")\n",
    "\n",
    "df['composite_score'] = (\n",
    "    0.5 * df['viewCount_norm'] +  # Weight for viewCount\n",
    "    0.3 * df['likeCount_norm'] +  # Weight for likeCount\n",
    "    0.2 * df['commentCount_norm']  # Weight for commentCount\n",
    ")\n",
    "\n",
    "top_videos = df.nlargest(4, 'composite_score')\n",
    "\n",
    "for index, video in top_videos.iterrows():\n",
    "    print(f\"Rank: {index + 1}\")\n",
    "    print(f\"Title: {video['v_title']}\")\n",
    "    print(f\"Link: {video['v_link']}\")\n",
    "    print(f\"Composite Score: {video['composite_score']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COURSERA COURSES RECOMMENDATION ON LEARNING ANY SKILL RECOMMENDED FROM ROADMAP\n",
    "\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import time\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Initialize Chrome options\n",
    "options = Options()\n",
    "\n",
    "# Set the correct path for Chrome if needed (not required in most cases)\n",
    "chrome_path = \"C:/Program Files/Google/Chrome/Application/chrome.exe\"\n",
    "options.binary_location = chrome_path\n",
    "\n",
    "# Initialize Chrome WebDriver properly\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Open Naukri.com\n",
    "driver.get(\"https://www.coursera.org/\")  # Print page title to verify success\n",
    "\n",
    "# Close driver after use\n",
    "\n",
    "# ENTER THE ROLE TO SEARCH FOR\n",
    "\n",
    "\n",
    "button = driver.find_element(By.XPATH, '//*[@id=\"rendered-content\"]/div/header/div/div/div/div[1]/div/div/div/div/div[2]/span[1]/button').click()\n",
    "\n",
    "input_search = driver.find_element(By.XPATH, '//*[@id=\"search-autocomplete-input\"]')\n",
    "\n",
    "input_search.send_keys('Web Development')\n",
    "\n",
    "input_search.send_keys(Keys.ENTER) \n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "posting = soup.find_all('div', class_ = 'cds-ProductCard-base cds-ProductCard-list css-2kdvnl')\n",
    "\n",
    "len(posting)\n",
    "\n",
    "# CODE FOR 1 SINGLE PAGE \n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "df = pd.DataFrame(columns=['Free_Paid', 'Link', 'Name', 'From', 'SkillsGain', 'rating', 'reviews', 'ExpType_Time'])\n",
    "data_list = []  # Store data in a list first (efficient way)\n",
    "\n",
    "for post in posting:\n",
    "    # Extract Job Title and Link\n",
    "    row1_div = post.find('div', class_='cds-ProductCard-statusTags cds-ProductCard-statusTagsSimple')  # Removed extra space in class name\n",
    "    if row1_div:\n",
    "        span_tag = row1_div.find('span')  # Find the <a> tag inside row1\n",
    "        free_paid = span_tag.text.strip() if span_tag else None  # Extract text (job title)\n",
    "        # href = span_tag['href'] if a_tag and 'href' in a_tag.attrs else None  # Extract href link\n",
    "    else:\n",
    "        free_paid = 'Paid'  # Default if div not found\n",
    "        \n",
    "    # link_element = driver.find_element(By.XPATH, '//a[contains(@class, \"cds-CommonCard-titleLink\")]')\n",
    "    # link = link_element.get_attribute(\"href\")\n",
    "    \n",
    "    link = post.find('a', class_ = 'cds-CommonCard-titleLink').get('href')\n",
    "    full_link = f\"https://www.coursera.org{link}\"\n",
    "    \n",
    "    name = post.find('h3', class_ = 'cds-CommonCard-title css-6ecy9b').text\n",
    "\n",
    "    # Extract Company Name\n",
    "    fromC = post.find('p', class_='cds-ProductCard-partnerNames css-vac8rf').text  # Removed extra space in class name\n",
    "    # if fromC:\n",
    "    #     a_tag = fromC.find('a')  # Find <a> inside div (ignoring span class)\n",
    "    #     fromC = a_tag.text.strip() if a_tag else None  # Extract text from <a>\n",
    "    # else:\n",
    "    #     fromC = None  # If div is not found\n",
    "\n",
    "    # Extract Experience, Salary, Location\n",
    "    SkillsGain = post.find('div', class_='cds-ProductCard-body').text.strip()\n",
    "    skills_list = [skill.strip() for skill in SkillsGain.split(',')]  # If skills are comma-separated\n",
    "\n",
    "    \n",
    "    # rating_element = post.find('div', class_='cds-RatingStat-sizeLabel css-1i7bybc').text\n",
    "    \n",
    "    rating_element = post.find('div', class_='cds-CommonCard-ratings')\n",
    "    text = rating_element.get_text(separator=' ', strip=True)\n",
    "    parts = text.split('·')  # The \"·\" character separates rating and reviews\n",
    "    rating = parts[0].strip()  # \"4.7 Rating, 4.7 out of 5 stars\"\n",
    "    reviews = parts[1].strip() if len(parts) > 1 else None\n",
    "\n",
    "    # Extract Skills\n",
    "    # expTime = post.find('div', class_='cds-CommonCard-metadata').text  # Removed extra space in class name\n",
    "    \n",
    "    expTime = post.find('div', class_='cds-CommonCard-metadata').text.strip()\n",
    "    expTime_list = [expTime.strip() for expTi in expTime.split(',')] \n",
    "\n",
    "    # Append data to the list\n",
    "    data_list.append({\n",
    "        'Free_Paid': free_paid,\n",
    "        'Link': full_link,\n",
    "        'Name': name,\n",
    "        'From': fromC,\n",
    "        'SkillsGain': skills_list,  # Fixed issue (location now assigned correctly)\n",
    "        'rating': rating,\n",
    "        'reviews': reviews,  # Fixed issue (was duplicated before)\n",
    "        'ExpType_Time': expTime_list\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.concat([df, pd.DataFrame(data_list)], ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"CourseraSD.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Data extraction completed and saved to CSV.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRAPING NAUKRI.COM FOR JOB POSTINGS THAT CAN BE USEFUL FOR USERS \n",
    "\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Initialize Chrome options\n",
    "options = Options()\n",
    "\n",
    "# Set the correct path for Chrome if needed (not required in most cases)\n",
    "chrome_path = \"C:/Program Files/Google/Chrome/Application/chrome.exe\"\n",
    "options.binary_location = chrome_path\n",
    "\n",
    "# Initialize Chrome WebDriver properly\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Open Naukri.com\n",
    "driver.get(\"https://www.naukri.com/\")  # Print page title to verify success\n",
    "\n",
    "# Close driver after use\n",
    "\n",
    "# ENTER THE ROLE TO SEARCH FOR\n",
    "\n",
    "\n",
    "input_search = driver.find_element(By.XPATH, '//*[@id=\"root\"]/div[7]/div/div/div[1]/div/div/div[1]/div[1]/div/input')\n",
    "\n",
    "input_search.send_keys('App Development')\n",
    "\n",
    "button = driver.find_element(By.XPATH, '//*[@id=\"root\"]/div[7]/div/div/div[6]').click()\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "posting = soup.find_all('div', class_ = 'srp-jobtuple-wrapper')\n",
    "\n",
    "len(posting)\n",
    "\n",
    "\n",
    "# # CODE FOR 5 PAGES \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# # Initialize WebDriver\n",
    "# driver = webdriver.Chrome()  # Ensure you have the correct driver installed\n",
    "\n",
    "df = pd.DataFrame(columns=['Link', 'Name', 'CompName', 'loc', 'sal', 'exp', 'Skills'])\n",
    "data_list = []  # Store data in a list first (efficient way)\n",
    "\n",
    "# Loop through 5 pages\n",
    "for page in range(5):\n",
    "    time.sleep(2)  # Let the page load\n",
    "\n",
    "    # Get the page source and parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    posting = soup.find_all('div', class_ = 'srp-jobtuple-wrapper')\n",
    "    \n",
    "    for post in posting:\n",
    "        # Extract Job Title and Link\n",
    "        row1_div = post.find('div', class_='row1')\n",
    "        if row1_div:\n",
    "            a_tag = row1_div.find('a')\n",
    "            title = a_tag.text.strip() if a_tag else None\n",
    "            href = a_tag['href'] if a_tag and 'href' in a_tag.attrs else None\n",
    "        else:\n",
    "            title, href = None, None\n",
    "\n",
    "        # Extract Company Name\n",
    "        Compname = post.find('div', class_='row2')\n",
    "        if Compname:\n",
    "            a_tag = Compname.find('a')\n",
    "            Compname = a_tag.text.strip() if a_tag else None\n",
    "        else:\n",
    "            Compname = None\n",
    "\n",
    "        # Extract Experience, Salary, Location\n",
    "        row3_div = post.find('div', class_='row3')\n",
    "        experience, salary, location = None, None, None\n",
    "\n",
    "        if row3_div:\n",
    "            spans = row3_div.find_all('span')\n",
    "\n",
    "            for span in spans:\n",
    "                title_attr = span.get('title', '').strip()\n",
    "\n",
    "                if 'Yrs' in title_attr:\n",
    "                    experience = title_attr\n",
    "                elif 'disclosed' in title_attr or 'PA' in title_attr:\n",
    "                    salary = title_attr\n",
    "                else:\n",
    "                    location = title_attr\n",
    "\n",
    "        # Extract Skills\n",
    "        row5_div = post.find('div', class_='row5')\n",
    "        skills = [li.text.strip() for li in row5_div.find_all('li')] if row5_div else []\n",
    "\n",
    "        # Append data to the list\n",
    "        data_list.append({\n",
    "            'Link': href,\n",
    "            'Name': title,\n",
    "            'CompName': Compname,\n",
    "            'loc': location,\n",
    "            'sal': salary,\n",
    "            'exp': experience,\n",
    "            'Skills': skills\n",
    "        })\n",
    "\n",
    "    # Try to click the \"Next\" button to go to the next page\n",
    "    try:\n",
    "        next_button = driver.find_element(By.CSS_SELECTOR, '#lastCompMark > a:nth-child(4)')\n",
    "        next_button.click()\n",
    "        time.sleep(2)  # Allow time for the next page to load\n",
    "    except:\n",
    "        print(\"No Next button found or reached last page.\")\n",
    "        break  # Stop if no next button\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.concat([df, pd.DataFrame(data_list)], ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"naukri_jobsSDENew.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Data extraction completed and saved to CSV.\")\n",
    "\n",
    "# # Close the driver\n",
    "# driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATING PERCENT MATCHING THAT WILL BE USEFUL FOR USER TO APPLY FOR JOBS WHERE HE/SHE HAS MORE CHANCES TO BE SELECTED IN\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import re\n",
    "import ast  # To parse list-like structures\n",
    "\n",
    "# Function to extract text from PDF resume\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            extracted_text = page.extract_text()\n",
    "            if extracted_text:\n",
    "                text += extracted_text + \" \"\n",
    "    return text.strip()\n",
    "\n",
    "# Function to extract skills from resume text\n",
    "def extract_skills(text):\n",
    "    skill_keywords = {\n",
    "        \"python\", \"java\", \"sql\", \"machine learning\", \"deep learning\",\n",
    "        \"pytorch\", \"javascript\", \"node.js\", \"react.js\", \"hadoop\",\n",
    "        \"spark\", \"tableau\", \"power bi\", \"tensorflow\", \"nlp\", \"aws\",\n",
    "        \"pandas\", \"numpy\", \"matplotlib\", \"seaborn\", \"data science\", \"mongodb\"\n",
    "    }\n",
    "    words = set(re.findall(r'\\b[a-zA-Z0-9+.-]+\\b', text.lower()))\n",
    "    return skill_keywords.intersection(words)\n",
    "\n",
    "# Extract skills from resume\n",
    "resume_text = extract_text_from_pdf(\"/software-engineer-resume-example.pdf\")\n",
    "resume_skills = extract_skills(resume_text)\n",
    "\n",
    "print(\"Extracted Resume Skills:\", resume_skills)  # Debugging output\n",
    "\n",
    "# Read job CSV\n",
    "job_df = pd.read_csv(\"/naukri_jobs_app dev.csv\")\n",
    "\n",
    "# Function to clean and extract job skills\n",
    "def extract_job_skills(skill_str):\n",
    "    if not isinstance(skill_str, str) or not skill_str.strip():\n",
    "        return set()\n",
    "\n",
    "    try:\n",
    "        skill_data = ast.literal_eval(skill_str)  # Convert string representation of lists/sets into actual sets\n",
    "        if isinstance(skill_data, list):\n",
    "            skill_data = set(skill_data)\n",
    "        elif not isinstance(skill_data, set):\n",
    "            skill_data = {skill_data}\n",
    "        return set(map(str.lower, map(str.strip, skill_data)))  # Ensure lowercase & remove spaces\n",
    "    except (SyntaxError, ValueError):\n",
    "        return set()  # Return empty set if parsing fails\n",
    "\n",
    "# Apply skill extraction to CSV\n",
    "job_df[\"Skills\"] = job_df[\"Skills\"].apply(extract_job_skills)\n",
    "\n",
    "# Debugging: Print cleaned job skills\n",
    "print(\"First few job skills from CSV:\\n\", job_df[\"Skills\"].head())\n",
    "\n",
    "# Function to calculate match percentage\n",
    "def calculate_match(job_skills):\n",
    "    if not job_skills:\n",
    "        return 0\n",
    "\n",
    "    match_count = len(resume_skills.intersection(job_skills))\n",
    "    return (match_count / len(job_skills)) * 100 if job_skills else 0\n",
    "\n",
    "# Apply function to DataFrame\n",
    "job_df[\"Match Percentage\"] = job_df[\"Skills\"].apply(calculate_match)\n",
    "\n",
    "# Print results\n",
    "for _, row in job_df.iterrows():\n",
    "    print(f\"{row['CompName']} ---- {row['Name']}: {row['Match Percentage']:.2f}% match\")\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import re\n",
    "import ast  # To parse list-like structures\n",
    "\n",
    "# Function to extract text from PDF resume\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            extracted_text = page.extract_text()\n",
    "            if extracted_text:\n",
    "                text += extracted_text + \" \"\n",
    "    return text.strip()\n",
    "\n",
    "# Function to extract skills from resume text\n",
    "def extract_skills(text):\n",
    "    skill_keywords = {\n",
    "        \"python\", \"java\", \"sql\", \"machine learning\", \"deep learning\",\n",
    "        \"pytorch\", \"javascript\", \"node.js\", \"react.js\", \"hadoop\",\n",
    "        \"spark\", \"tableau\", \"power bi\", \"tensorflow\", \"nlp\", \"aws\",\n",
    "        \"pandas\", \"numpy\", \"matplotlib\", \"seaborn\", \"data science\", \"mongodb\"\n",
    "    }\n",
    "    words = set(re.findall(r'\\b[a-zA-Z0-9+.-]+\\b', text.lower()))\n",
    "    return skill_keywords.intersection(words)\n",
    "\n",
    "# Extract skills from resume\n",
    "resume_text = extract_text_from_pdf(\"/software-engineer-resume-example.pdf\")\n",
    "resume_skills = extract_skills(resume_text)\n",
    "\n",
    "print(\"Extracted Resume Skills:\", resume_skills)  # Debugging output\n",
    "\n",
    "# Read job CSV\n",
    "job_df = pd.read_csv(\"/naukri_jobs_app dev.csv\")\n",
    "\n",
    "# Function to clean and extract job skills\n",
    "def extract_job_skills(skill_str):\n",
    "    if not isinstance(skill_str, str) or not skill_str.strip():\n",
    "        return set()\n",
    "\n",
    "    try:\n",
    "        skill_data = ast.literal_eval(skill_str)  # Convert string representation of lists/sets into actual sets\n",
    "        if isinstance(skill_data, list):\n",
    "            skill_data = set(skill_data)\n",
    "        elif not isinstance(skill_data, set):\n",
    "            skill_data = {skill_data}\n",
    "        return set(map(str.lower, map(str.strip, skill_data)))  # Ensure lowercase & remove spaces\n",
    "    except (SyntaxError, ValueError):\n",
    "        return set()  # Return empty set if parsing fails\n",
    "\n",
    "# Apply skill extraction to CSV\n",
    "job_df[\"Skills\"] = job_df[\"Skills\"].apply(extract_job_skills)\n",
    "\n",
    "# Debugging: Print cleaned job skills\n",
    "print(\"First few job skills from CSV:\\n\", job_df[\"Skills\"].head())\n",
    "\n",
    "# Function to calculate match percentage\n",
    "def calculate_match(job_skills):\n",
    "    if not job_skills:\n",
    "        return 0\n",
    "\n",
    "    match_count = len(resume_skills.intersection(job_skills))\n",
    "    return (match_count / len(job_skills)) * 100 if job_skills else 0\n",
    "\n",
    "# Apply function to DataFrame\n",
    "job_df[\"Match Percentage\"] = job_df[\"Skills\"].apply(calculate_match)\n",
    "\n",
    "# Print results\n",
    "for _, row in job_df.iterrows():\n",
    "    print(f\"{row['CompName']} ---- {row['Name']}: {row['Match Percentage']:.2f}% match\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
